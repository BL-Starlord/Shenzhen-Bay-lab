---
title: "Hotspot"
author: "Liangxiao LI"
date: "2023-07-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Load library
Parameter change: at row 405，row 776 (aplha)
```{r cars, echo=FALSE, include=FALSE}
library(sp)
library(spgwr)
#library(spdep)
library(readxl)
#library(spatialreg)
library(gstat)
library(stats)
library(dbscan)
library(cluster)
library(ggplot2)
library(scatterplot3d)
library(rgl)
library(mclust)
library(plotly)
library(dplyr)
library(FNN)
library(matrixStats)
library(tidyr)
library(scales)
library(factoextra)
library(spatstat)
library(clValid)
library(factoextra)
library(fpc)
library(NbClust)
library(readxl)
library(plot3D)
library(DT)
# 加载ks包，用于进行Kolmogorov-Smirnov检验
library(ks)
library(gridExtra)

```

## Load data

Load data with spatial coordinates and mutation rates

```{r, echo=FALSE}
#Pipe
#data<-as.data.frame(read.csv("data1.csv"))

#Xiao
setwd("/Users/ryan/Documents/GitHub/Shenzhen Bay lab/spike_protein_3structures_markerd_with_variants_from_cncb")
data <- read_excel("6vxx_variants.xls")


data<-na.omit(data) #去除na值
data_matrix<-as.matrix(data)
data2<-data[1:972,c("X","Y","Z","virusPercent")]

```

## Prove necessary to do normalise


```{r, echo=FALSE}
# generate random 3-d coordinate
#Pipe
#data <- read.csv("data1.csv")
#Xiao
setwd("/Users/ryan/Documents/GitHub/Shenzhen Bay lab/spike_protein_3structures_markerd_with_variants_from_cncb")
data <- read_excel("6vxx_variants.xls")

sub_data<-data[1:972,c("X","Y","Z","virusPercent")]
sub_data_1<-data[1:972,c("X","Y","Z")]
# 计算每个维度的基本统计信息
summary(sub_data)

```

##Some plots


##Prove necessary to do normalization

Method 1 : 2 sample t test

Caculate each dimension's mean and standard error

mean
```{r, echo=FALSE}
apply(sub_data,2,mean)

```

standard error
```{r}
apply(sub_data,2,sd)
```



H0 : equal mean
H1 : not equal mean

if p-value small, then 2 covariates has significant difference between mean, which means it requires normalization.(scale)


```{r, echo=FALSE}

t.test(sub_data[, 1], sub_data[, 2])
t.test(sub_data[, 1], sub_data[, 3])
t.test(sub_data[, 2], sub_data[, 3])

```

Method 2: correlation analysis

```{r, echo=FALSE}

#方差分析空间差异性
data_df<-as.data.frame(sub_data_1)

# 计算皮尔逊相关系数
cor_mat <- cor(data_df, method = "pearson")
# 查看相关系数矩阵
print(cor_mat)
#相关系数的范围在 -1 到 1 之间,
#绝对值越大则表示相关性越强，而正负号则表示相关性的方向

for (i in 1:(ncol(data_df) - 1)) {
  for (j in (i + 1):ncol(data_df)) {
    cor_test <- cor.test(data_df[, i], data_df[, j], method = "pearson")
    cat("correlation between", names(data_df)[i], "and", names(data_df)[j], "is", cor_test$estimate, "with p-value", cor_test$p.value, "\n")
  }
}


# 计算肯德尔等级相关系数
cor_mat_2 <- cor(data_df, method = "kendall")
# 查看相关系数矩阵
print(cor_mat_2)
# 计算多个变量之间的肯德尔等级相关系数及其显著性水平
for (i in 1:(ncol(data_df) - 1)) {
  for (j in (i + 1):ncol(data_df)) {
    cor_test <- cor.test(data_df[, i], data_df[, j], method = "kendall")
    cat("correlation between", names(data_df)[i], "and", names(data_df)[j], "is", cor_test$estimate, "with p-value", cor_test$p.value, "\n")
  }
}


#如果显著性水平非常小（通常小于 0.05），
#则我们可以认为相关性是显著的，也就是说，在总体水平下，
#这两个变量之间存在显著的相关性。

```

Therefore, we need to do normalization

#Normalization validation

It's able to normalize if the data follows same distribution before and after the normalization.

First normalise x,y,z
```{r}
#抽提坐标数据
xyz<-data2[,c("X","Y","Z")]
#标准化xyz数据
xyz_norm<-scale(xyz)
# 将标准化处理过的坐标和突变频率组合为特征向量
feature_vector <- cbind(xyz_norm, data2$virusPercent)

data2 <- data.frame(data2)

```

Here we prove that x,y,z follows this rule.

```{r}
plot3d(feature_vector[,1], feature_vector[,2], feature_vector[,3], col = "blue", size = 2)
plot3d(data2[,1], data2[,2], data2[,3], col = "blue", size = 2)
```

```{r}

#原始的xyz和标准化后的xyz的数据范围和分布
# 查看原始数据的数据范围和分布情况
summary(data2[, 1:3])  # 显示前三列（x、y、z）的数据范围和分布情况
hist(data2[, 1])      # 绘制x列的直方图
hist(data2[, 2])      # 绘制y列的直方图
hist(data2[, 3])      # 绘制z列的直方图

```

```{r}
# 查看标准化后的数据的数据范围和分布情况
summary(feature_vector[, 1:3])  # 显示前三列（x_norm、y_norm、z_norm）的数据范围和分布情况
hist(feature_vector[, 1])      # 绘制x_norm列的直方图
hist(feature_vector[, 2])      # 绘制y_norm列的直方图
hist(feature_vector[, 3])      # 绘制z_norm列的直方图
```

```{r}

# 对比前后x列数据的分布
ks.test(data2[, 1], feature_vector[, 1])  # 进行Kolmogorov-Smirnov检验，比较x列的分布

# 对比前后y列数据的分布
ks.test(data2[, 2], feature_vector[, 2])  # 进行Kolmogorov-Smirnov检验，比较y列的分布

# 对比前后z列数据的分布
ks.test(data2[, 3], feature_vector[, 3])  # 进行Kolmogorov-Smirnov检验，比较z列的分布

```

Here we prove that x,y,z,vp follows the rule

```{r}
# 原始数据的密度图
p1 <- ggplot(data2, aes(x = X)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("X") + ylab("Density") + ggtitle("Original X Density Distribution")

p2 <- ggplot(data2, aes(x = Y)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("Y") + ylab("Density") + ggtitle("Original Y Density Distribution")

p3 <- ggplot(data2, aes(x = Z)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("Z") + ylab("Density") + ggtitle("Original Z Density Distribution")
feature_vector<-data.frame(feature_vector)
# 标准化后的密度图
p4 <- ggplot(feature_vector, aes(x = X)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("X") + ylab("Density") + ggtitle("Standardized X Density Distribution")

p5 <- ggplot(feature_vector, aes(x = Y)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("Y") + ylab("Density") + ggtitle("Standardized Y Density Distribution")

p6 <- ggplot(feature_vector, aes(x = Z)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("Z") + ylab("Density") + ggtitle("Standardized Z Density Distribution")

# 将六张图合并为两行三列,形状，位置，峰度和偏度
grid.arrange(p1, p2, p3, p4, p5, p6, nrow = 2, ncol = 3)

```

```{r}
# 循环绘制所有的二维多变量密度图
# 标准化前的二维密度图
library(ggplot2)
p1 <- ggplot(data2, aes(X, Y)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "X", y = "Y") +
  theme_bw()

p2 <- ggplot(data2, aes(X, Z)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "X", y = "Y") +
  theme_bw()

p3 <- ggplot(data2, aes(Y, Z)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "Y", y = "Z") +
  theme_bw()
#标准化后的二维变量密度图
p4 <- ggplot(feature_vector, aes(X, Y)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "X", y = "Y") +
  theme_bw()

p5 <- ggplot(feature_vector, aes(X, Z)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "X", y = "Z") +
  theme_bw()

p6 <- ggplot(feature_vector, aes(Y, Z)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "Y", y = "Z") +
  theme_bw()

# 将六张图合并为两行三列,形状，位置，峰度和偏度
grid.arrange(p1, p2, p3, p4, p5, p6, nrow = 2, ncol = 3)

```

Transformation method 1:
```{r,echo=FALSE}

#转化方法1：同时标准化处理
#抽提突变频率
w<-data2[,c("virusPercent")]
#标准化xyz数据
w_norm<-scale(w)
# 将标准化处理过的坐标和突变频率组合为特征向量
scale_vector <- cbind(xyz_norm, w_norm)
#观察突变频率标准化前后的数据分布
p1 <- ggplot(data2, aes(x = virusPercent)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("virusPercent") + ylab("Density") + ggtitle("Original vP Density Distribution")
p1<-p1+xlim(0,0.02)

scale_vector<-data.frame(scale_vector)

p2 <- ggplot(scale_vector, aes(x = V4)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("virusPercent") + ylab("Density") + ggtitle("standardized vP Density Distribution")
p2<-p2+xlim(-0.2,-0.1)

# 将两张图合并为一行二列,形状，位置，峰度和偏度
grid.arrange(p1, p2, nrow = 1, ncol = 2)

#直方图
hist(data2[,4])
hist(scale_vector[,4])

# 对比前后突变频率的分布
ks.test(data2[, 4], scale_vector[, 4])  # 进行Kolmogorov-Smirnov检验，比较z列的分布

```


Transformation Method 2:

```{r,echo=FALSE}
#转化方法2：特征放缩
# 最小-最大缩放x，y，z坐标值
data2$x <- (data2$X - min(data2$X)) / (max(data2$X) - min(data2$X))
data2$y <- (data2$Y - min(data2$Y)) / (max(data2$Y) - min(data2$Y))
data2$z <- (data2$Z - min(data2$Z)) / (max(data2$Z) - min(data2$Z))
data2$mutation <- (data2$virusPercent - min(data2$virusPercent)) / (max(data2$virusPercent) - min(data2$virusPercent))

# 构建新的数据框只存放缩后的四列数据
new_data <- data.frame(x = data2$x, y = data2$y, z = data2$z, mutation = data2$mutation)

#验证特征放缩前后数据分布相似性

# 创建3D散点图
plot3d(new_data[,1], new_data[,2], new_data[,3], col = "blue", size = 2)

# 绘制散点图矩阵
pairs(data2[, c("X", "Y", "Z", "virusPercent")], main = "Scatterplot Matrix before")
pairs(new_data[,c("x","y","z","mutation")],main = "Scatterplot Matrix after")

# 比较缩放前后数据的直方图
par(mfrow = c(2, 4))   # 将图形区域划分为2行4列
hist(data2$virusPercent, main = "Mut Before Scale", xlab = "Mutation")
hist(data2$X, main = "X Before Scale", xlab = "X")
hist(data2$Y, main = "Y Before Scale", xlab = "Y")
hist(data2$Z, main = "Z Before Scale", xlab = "Z")
hist(new_data$mutation, main = "Mut After Scale", xlab = "Mutation")
hist(new_data$x, main = "X After Scale", xlab = "X")
hist(new_data$y, main = "Y After Scale", xlab = "Y")
hist(new_data$z, main = "Z After Scale", xlab = "Z")

# 比较缩放前后数据的密度图
par(mfrow = c(2, 4))   # 将图形区域划分为2行4列
plot(density(data2$virusPercent), main = "Mut Before Scale", xlab = "Mutation")
plot(density(data2$X), main = "X Before Scaling", xlab = "X")
plot(density(data2$Y), main = "Y Before Scaling", xlab = "Y")
plot(density(data2$Z), main = "Z Before Scaling", xlab = "Z")
plot(density(new_data$mutation), main = "Mut After Scale", xlab = "Mutation")
plot(density(new_data$x), main = "X After Scale", xlab = "X")
plot(density(new_data$y), main = "Y After Scale", xlab = "Y")
plot(density(new_data$z), main = "Z After Scale", xlab = "Z")

# 比较缩放前后数据的QQ图
par(mfrow = c(2, 4))   # 将图形区域划分为2行4列
qqnorm(data2$virusPercent, main = "Mut Before Scale")
qqline(data2$virusPercent)
qqnorm(data2$X, main = "X Before Scale")
qqline(data2$X)
qqnorm(data2$Y, main = "Y Before Scale")
qqline(data2$Y)
qqnorm(data2$Z, main = "Z Before Scale")
qqline(data2$Z)
qqnorm(new_data$mutation, main = "Mut After Scale")
qqline(new_data$mutation)
qqnorm(new_data$x, main = "X After Scale")
qqline(new_data$x)
qqnorm(new_data$y, main = "Y After Scale")
qqline(new_data$y)
qqnorm(new_data$z, main = "Z After Scale")
qqline(new_data$z)

```


#K-means cluster

Clustering - Model validation (Find best parameter)
```{r}

#Model validation - find best parameter
index <- data.frame()

#标记最大几个值
topnum = 20

#K-num iteration
Kmin <- 10
Kmax <- 180
iteration <- 20 #iteration number per k num 

for (i in Kmin:Kmax) {
  
  #nstart = 20 means perform k = i for 20 times and select the best cluster outcome among them
  kmeans_result <- kmeans(new_data, centers = i, nstart = iteration)
  
  wss <- sum(kmeans_result$withinss)
  
  km_stats <- cluster.stats(dist(data2),  kmeans_result$cluster)

  # Dunn index
  dunn <- km_stats$dunn
  # Calinski-Harabasz index
  ch <- km_stats$ch
  # Average silhouette width
  sil <- km_stats$avg.silwidth
  #Entropy index
  ent <- km_stats$entropy

  
  
  # number of noise points :Noise points are data points that do not belong to any specific cluster or are considered outliers
  km_stats$noisen
  # vector of clusterwise within cluster average distances. (这个是scaled version的average distance， non-scaled version到时候还得再算一下)
  km_stats$average.distance
  # Calculate the separation matrix. provides a summary of the between-cluster distances and can help identify clusters that are well-separated or overlapping.
  #The separation matrix is a symmetric matrix that quantifies the degree of separation between clusters based on some distance measure. 
  separation_matrix <- km_stats$separation.matrix
  
  index <- rbind(index, c(dunn, ch, sil, ent,wss))
  
  }

```

```{r}


colnames(index) <- c("Dunn", "Calinski-Harabasz", "Average_Silhouette_Width", "Entropy","Wss")

#par(mfrow = c(2, 2))

# Line chart for Dunn Index
plot(index$Dunn, type = "l", xlab = "Observation", ylab = "Dunn Index", main = "Dunn Index",xlim = c(0, length(index$Dunn) +1), ylim = c(min(index$Dunn)*0.95, max(index$Dunn)*1.2 ))

#创建表
top_dun <- data.frame()

# 标记最大值，并加进表里
sorted <- sort(index$Dunn, decreasing = TRUE)
top_three <- head(sorted, topnum)

print(top_three)

for (i in 1:length(top_three)) {
  
  num <- which(index$Dunn == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  
  for (n in num){
    points(x = n, y= top_three[i], col = "red", pch = 19)
  }
  
  top_dun <- rbind(top_dun, c(i,top_three[i]) )
}
colnames(top_dun) <- c("K-Num", "Dunn")
datatable(top_dun)

```


```{r}

# Line chart for Calinski-Harabasz Index
plot(index$`Calinski-Harabasz`, type = "l", xlab = "Observation", ylab = "Calinski-Harabasz Index", main = "Calinski-Harabasz Index",xlim = c(0, length(index$`Calinski-Harabasz`) +1), ylim = c(min(index$`Calinski-Harabasz`)*0.95, max(index$`Calinski-Harabasz`)*1.2 ))

#创建表
top_ch <- data.frame()

# 标记最大值
sorted <- sort(index$`Calinski-Harabasz`, decreasing = TRUE)
top_three <- head(sorted, topnum)

for (i in 1:length(top_three)) {
  num <- which(index$`Calinski-Harabasz` == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  
  for (n in num){
    points(x = n, y= top_three[i], col = "red", pch = 19)
  }
  top_ch <- rbind(top_ch, c(num,top_three[i]) )
}

colnames(top_ch) <- c("K-Num", "CH")
datatable(top_ch)

```


```{r}

# Line chart for Average Silhouette Width
plot(index$`Average_Silhouette_Width`, type = "l", xlab = "Observation", ylab = "Average Silhouette Width", main = "Average Silhouette Width",xlim = c(0, length(index$`Average_Silhouette_Width`) +1), ylim = c(min(index$`Average_Silhouette_Width`)*0.95, max(index$`Average_Silhouette_Width`)*1.2))

#创建表
top_ASW <- data.frame()

# 标记最大值
sorted <- sort(index$`Average_Silhouette_Width`, decreasing = TRUE)
top_three <- head(sorted, topnum)

for (i in 1:length(top_three)) {
  num <- which(index$`Average_Silhouette_Width` == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  
  for (n in num){
    points(x = n, y= top_three[i], col = "red", pch = 19)
  }
  top_ASW <- rbind(top_ASW, c(num,top_three[i]) )
}

colnames(top_ASW) <- c("K-Num", "CH")
datatable(top_ASW)

```


```{r}

# Line chart for Entropy
plot(index$Entropy, type = "l", xlab = "Observation", ylab = "Entropy", main = "Entropy",xlim = c(0, length(index$Entropy) +1), ylim = c(min(index$Entropy)*0.95, max(index$Entropy)*1.2))


# 标记最大值
sorted <- sort(index$Entropy, decreasing = FALSE)
top_three <- head(sorted, topnum)

for (i in 1:length(top_three)) {
  num <- which(index$Entropy == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  
  for (n in num){
    points(x = n, y= top_three[i], col = "red", pch = 19)
  }
}
```


```{r}

# Line chart for Wss
plot(index$Wss, type = "l", xlab = "Observation", ylab = "Wss", main = "Elbow method",xlim = c(0, length(index$Wss) +1), ylim = c(min(index$Wss)*0.95, max(index$Wss)*1.2))


# 标记最大值
sorted <- sort(index$Wss, decreasing = TRUE)
top_three <- head(sorted,topnum)

for (i in 1:length(top_three)) {
  num <- which(index$Wss == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  for (n in num){
    points(x = n, y= top_three[i], col = "red", pch = 19)
  }
}


# 将聚类结果添加到数据框中
#data2$cluster <- as.factor(kmeans_result$cluster)


```



Dunn:the higher the better
(well-separated and compact)

CH:the higher the better
(well-separated and compact)

S:The higher the better
(well-separated and compact)

Entropy: the lower the better

Wss: the lower the better

weighted mean score = Dunn percentage + CH percentage + S percentage + entropy percentage + Wss percentage

```{r}

D <- index$Dunn
A <- index$`Average_Silhouette_Width`
CH <- index$`Calinski-Harabasz`
E <- index$Entropy
Wss <- index$Wss


#创建表
WS <- data.frame()

for (i in 1:length(D)){
  
  weight <- D[i]/max(D) + A[i]/max(A) + CH[i]/max(CH) + (1-E[i]/max(E)) + (1-Wss[i]/max(Wss))
  
  WS <- rbind(WS, c(i+Kmin -1 ,weight) )
  
}

colnames(WS) <- c("K-Num", "Weighted score")
datatable(WS)

# Line chart for WS
plot(x=WS$`K-Num` ,y=WS$`Weighted score`, type = "l", xlab = "K-num", ylab = "Weighted score", main = "Weighted score",xlim = c(Kmin, Kmin+length(WS$`Weighted score`)-1), ylim = c(min(WS$`Weighted score`)*0.95, max(WS$`Weighted score`)*1.2))

Best_K <- data.frame()

# 标记最大值
sorted <- sort(WS$`Weighted score`, decreasing = TRUE)
top_three <- head(sorted,topnum)

for (i in 1:length(WS$`Weighted score`)) {
  num <- which(WS$`Weighted score` == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  for (n in num){
    points(x = n +Kmin -1, y= top_three[i], col = "red", pch = 19)
    Best_K <- rbind(Best_K, c(i,n,top_three[i]) )
    
    
  }
}

colnames(Best_K) <- c("Best_descend", "K-num","Weighted score")
datatable(Best_K)

```


Therefore, we choose k = 122，k= 129,k=118 as our parameter in this circumstances.


```{r}

alpha = 0.05

k_validated <- data.frame()

for (k in Best_K[,2]){
#Permutation法(排名百分数假设检验+特征放缩处理)
# 计算原始数据的聚类结果
orig_clusters <- kmeans(new_data, k)
data3<-data[1:972,c("X","Y","Z","virusPercent")]
orig_means <- tapply(new_data$mutation, orig_clusters$cluster, mean)
#这一步是为了计算原始数据中每个类的平均突变频率所处的百分位数。
#具体地，中包含原始数据中每个类突变频率的均值，那么 rank(orig_means) 返回原始数据中每个类的突变频率均值的排名。
#然后，通过除以 length(orig_means)，即原始数据被分成的类数，可以计算出每个类的均值的排名的比例。
#最后，乘以 100 并四舍五入到两位小数，即可得到排名。
orig_pcts <- round(rank(orig_means)/length(orig_means)*100, 2)

# 进行2000次突变概率重排并计算每次聚类结果
n_permutations <- 2000
perm_results <- matrix(NA, n_permutations, length(orig_means))
for (i in 1:n_permutations) {
  perm_data <- data3
  perm_data[,ncol(data3)] <- sample(data3[,ncol(data3)], replace=FALSE)
  mut_scale <- (perm_data[,ncol(data3)] - min(perm_data[,ncol(data3)])) / (max(perm_data[,ncol(data3)]) - min(perm_data[,ncol(data3)]))
  sta_scale <- data.frame(x = data2$x,y = data2$y,z = data2$z,mutation = mut_scale)
  perm_clusters <- kmeans(sta_scale, k)
  
  
  colnames(sta_scale) <- c("x", "y","z","mutation")
  
  perm_means <- tapply(sta_scale$mutation, perm_clusters$cluster, mean)
  perm_pcts <- round(rank(perm_means)/length(perm_means)*100, 2)
  #这行代码是用来计算每个聚类在重排结果中的百分位数。
  #具体来说，rank()函数计算了每个聚类均值在重排结果中的排名，
  #然后除以重排结果的总数，并乘以100，
  #得到了每个聚类在重排结果中的百分位数。
  #最后，round()函数将结果保留两位小数并四舍五入。
  #这个百分位数可以用来评估原始聚类结果中每个聚类的可能性是否显著高于随机聚类结果，
  #在第六步计算p值时会用到。
  perm_results[i,] <- perm_pcts
  #将每次重排得到的聚类结果的百分位数保存在perm_results矩阵中。
  #具体来说，perm_results矩阵的每一行都对应一次重排结果，
  #每一列对应一个聚类的百分位数。
  #在for循环中，第i次重排得到的三个聚类的百分位数保存在perm_pcts向量中，
  #然后将perm_pcts向量赋值给perm_results矩阵的第i行，
  #从而将这次重排的聚类结果存储在perm_results矩阵中。
}
# 计算每个聚类的p值
#检验原始数据中每个类的均值是否显著高原始数据的均值。
#具体来说，这个假设检验的零假设是：原始数据中每个类的均值与原始数据的均值相等，即不存在显著差异。
#备择假设是：原始数据中每个类的均值显著高于原始数据中的均值
p_values <- numeric(length(orig_means))
for (i in 1:length(orig_means)) {
  #具体来说，对于原始数据中的每个聚类，该代码计算了重排结果中百分位数高于该聚类百分位数的比例（即p值）。
  #如果p值很小，说明原始聚类结果中该聚类的百分位数显著高于随机聚类结果，
  #从而支持该聚类是显著的。反之，如果p值很大，说明该聚类的百分位数可能与随机聚类结果中的百分位数相似，从而不支持该聚类是显著的
  p_values[i] <- mean(perm_results[,i] >= orig_pcts[i])
}
# 输出结果
#cat("k-num", k, "\n")
#cat("原始聚类结果的均值：", orig_means, "\n")
#cat("原始聚类结果的百分位数：", orig_pcts, "\n")
#cat("p值：", p_values, "\n")
#cat("原始聚类结果中最高的平均突变频率：", max(orig_means), "\n")
#cat("对应的p值：", p_values[which.max(orig_means)], "\n")


#筛选p值，p值过小的点的k才选中
if (p_values[which.max(orig_means)]< alpha) {
  # 如果 condition 为 TRUE，则执行这里的代码
  cat("对应的p值过小，拒绝H0")
  k_validated <- rbind(k_validated,k)
} else {
  # 如果 condition 为 FALSE，则执行这里的代码
  cat("对应的p值过大，不拒绝H0")
}



#每次重排都会对原始数据进行打乱，破坏原始数据之间的任何关联性，
#然后重新进行聚类，得到每个聚类的平均突变频率。
#重排得到的聚类结果是随机的，由于在每次重排中使用了相同的聚类算法和距离度量方法，
#因此重排结果可以用来估计原始聚类结果中每个聚类的可能性是否显著高于随机聚类结果。
#如果原始聚类结果中的每个聚类的百分位数显著高于随机聚类结果，那么说明该聚类在原始数据中可能具有显著的生物学意义

#我们想要看到的是1/3越多越好，2/3和3/3越少越好。所以当1000次中1/3的次数超过950时，即2/3和3/3的次数小于50时，可说明在该聚类相对排名的普适性。
#首先对原始数据进行聚类，计算每个聚类的平均突变频率，
#并将每个聚类的平均突变频率转换为对应的百分位数。
#然后，使用突变概率重排法重复执行聚类过程，得到1000个随机聚类结果，
#并计算每个聚类在随机聚类结果中的百分位数。
#最后，对于原始数据中的每个聚类，计算其在随机聚类结果中的排名比原始聚类结果高的比例，高意思就是平均突变率小。排名越高，对应类的平均突变频率越小。
#从而得到对应的p值。如果p值小于设定的显著性水平（通常是0.05），则拒绝原假设，原始聚类结果中该聚类不是随机出现的，否则接受原假设（即该聚类可能是由随机性产生的，没有显著性）

#如果原始数据聚类后的平均突变那一类的p值小于0.05的话，print出该类里的点
# 如果最高平均突变频率的类的p值小于0.05，则显示该类的点
if (p_values[which.max(orig_means)] < 0.05) {
  cat("原始聚类结果中平均突变频率最高的聚类中的点：\n")
  cat(paste("行号\tX\tY\tZ\tvirusPercent\n"))
  selected_rows <- which(orig_clusters$cluster == which.max(orig_means))
  for (i in selected_rows) {
    cat(paste(i, "\t", new_data[i, "x"], "\t", new_data[i, "y"], "\t", new_data[i, "z"], "\t", data[i, "virusPercent"], "\n"))
  }
  
  # 可视化所有点（用蓝色表示）和突出显示的点（用红色表示）
  significant_points <- which(orig_clusters$cluster == which.max(orig_means))  # 提取p值小于0.05的聚类中的所有数据点的索引
  print(significant_clusters)
  print(significant_points)
  # 提取p值小于0.05的聚类中的所有数据点的坐标
  significant_coordinates <- new_data[significant_points, c("x", "y", "z")]
  
  # 在三维点图中可视化所有数据点和p值小于0.05的聚类中的数据点
  fig <- plot_ly() %>%
    add_trace(data = new_data, x = ~x, y = ~y, z = ~z, type = "scatter3d", mode = "markers", name = "All Points") %>%
    add_trace(data = significant_coordinates, x = ~x, y = ~y, z = ~z, type = "scatter3d", mode = "markers", name = "Significant Clusters", marker = list(color = "red")) %>%
    layout(scene = list(xaxis = list(title = "X"), yaxis = list(title = "Y"), zaxis = list(title = "Z")))
  
  fig
}

#检验点的正确
cluster_data <- scale_vector[orig_clusters$cluster == which.max(orig_means), ]
print(cluster_data)

  
  
  
}

#筛选p值，p值过小的点的k才选中
if (length(k_validated) == 0) {
  temp <- 0
} else {
  # 如果 condition 为 FALSE，则执行这里的代码
  colnames(k_validated) <- c("K")

# 选取 K-num 与 K_validated 相同的行
Final_K <- Best_K[Best_K$`K-num` %in% k_validated$`K`, ]

datatable(Final_K)

}



```




```{r}

alpha = 0.05

k_validated <- data.frame()

for (k in Best_K[,2]){
  
#Permutation法(排名百分数假设检验+标准化处理)
# 计算原始数据的聚类结果
orig_clusters <- kmeans(scale_vector, k)
data3<-data[1:972,c("X","Y","Z","virusPercent")]
orig_means <- tapply(scale_vector$V4, orig_clusters$cluster, mean)
#这一步是为了计算原始数据中每个类的平均突变频率所处的百分位数。
#具体地，中包含原始数据中每个类突变频率的均值，那么 rank(orig_means) 返回原始数据中每个类的突变频率均值的排名。
#然后，通过除以 length(orig_means)，即原始数据被分成的类数，可以计算出每个类的均值的排名的比例。
#最后，乘以 100 并四舍五入到两位小数，即可得到排名。
orig_pcts <- round(rank(orig_means)/length(orig_means)*100, 2)

# 进行2000次突变概率重排并计算每次聚类结果
n_permutations <- 2000
perm_results <- matrix(NA, n_permutations, length(orig_means))
x_scale <- scale(data3[,1])
y_scale <- scale(data3[,2])
z_scale <- scale(data3[,3])
for (i in 1:n_permutations) {
  perm_data <- data3
  perm_data[,ncol(data3)] <- sample(data3[,ncol(data3)], replace=FALSE)
  mut_scale <- scale(perm_data[,ncol(data3)])
  sta_scale <- data.frame(x = x_scale,y = y_scale,z = z_scale,mutation = mut_scale)
  perm_clusters <- kmeans(sta_scale, k)
  
  colnames(sta_scale) <- c("x", "y","z","mutation")
  
  perm_means <- tapply(sta_scale$mutation, perm_clusters$cluster, mean)
  perm_pcts <- round(rank(perm_means)/length(perm_means)*100, 2)
  #这行代码是用来计算每个聚类在重排结果中的百分位数。
  #具体来说，rank()函数计算了每个聚类均值在重排结果中的排名，
  #然后除以重排结果的总数，并乘以100，
  #得到了每个聚类在重排结果中的百分位数。
  #最后，round()函数将结果保留两位小数并四舍五入。
  #这个百分位数可以用来评估原始聚类结果中每个聚类的可能性是否显著高于随机聚类结果，
  #在第六步计算p值时会用到。
  perm_results[i,] <- perm_pcts
  #将每次重排得到的聚类结果的百分位数保存在perm_results矩阵中。
  #具体来说，perm_results矩阵的每一行都对应一次重排结果，
  #每一列对应一个聚类的百分位数。
  #在for循环中，第i次重排得到的三个聚类的百分位数保存在perm_pcts向量中，
  #然后将perm_pcts向量赋值给perm_results矩阵的第i行，
  #从而将这次重排的聚类结果存储在perm_results矩阵中。
}
# 计算每个聚类的p值
#检验原始数据中每个类的均值是否显著高原始数据的均值。
#具体来说，这个假设检验的零假设是：原始数据中每个类的均值与原始数据的均值相等，即不存在显著差异。
#备择假设是：原始数据中每个类的均值显著高于原始数据中的均值
p_values <- numeric(length(orig_means))
for (i in 1:length(orig_means)) {
  #具体来说，对于原始数据中的每个聚类，该代码计算了重排结果中百分位数高于该聚类百分位数的比例（即p值）。
  #如果p值很小，说明原始聚类结果中该聚类的百分位数显著高于随机聚类结果，
  #从而支持该聚类是显著的。反之，如果p值很大，说明该聚类的百分位数可能与随机聚类结果中的百分位数相似，从而不支持该聚类是显著的
  p_values[i] <- mean(perm_results[,i] >= orig_pcts[i])
}
# 输出结果
#cat("k-num", k, "\n")
#cat("原始聚类结果的均值：", orig_means, "\n")
#cat("原始聚类结果的百分位数：", orig_pcts, "\n")
#cat("p值：", p_values, "\n")
#cat("原始聚类结果中最高的平均突变频率：", max(orig_means), "\n")
#cat("对应的p值：", p_values[which.max(orig_means)], "\n")

#筛选p值，p值过小的点的k才选中
if (p_values[which.max(orig_means)]< alpha) {
  # 如果 condition 为 TRUE，则执行这里的代码
  cat("对应的p值过小，拒绝H0")
  k_validated <- rbind(k_validated,k)
} else {
  # 如果 condition 为 FALSE，则执行这里的代码
  cat("对应的p值过大，不拒绝H0")
}




#每次重排都会对原始数据进行打乱，破坏原始数据之间的任何关联性，
#然后重新进行聚类，得到每个聚类的平均突变频率。
#重排得到的聚类结果是随机的，由于在每次重排中使用了相同的聚类算法和距离度量方法，
#因此重排结果可以用来估计原始聚类结果中每个聚类的可能性是否显著高于随机聚类结果。
#如果原始聚类结果中的每个聚类的百分位数显著高于随机聚类结果，那么说明该聚类在原始数据中可能具有显著的生物学意义

#我们想要看到的是1/3越多越好，2/3和3/3越少越好。所以当1000次中1/3的次数超过950时，即2/3和3/3的次数小于50时，可说明在该聚类相对排名的普适性。
#首先对原始数据进行聚类，计算每个聚类的平均突变频率，
#并将每个聚类的平均突变频率转换为对应的百分位数。
#然后，使用突变概率重排法重复执行聚类过程，得到1000个随机聚类结果，
#并计算每个聚类在随机聚类结果中的百分位数。
#最后，对于原始数据中的每个聚类，计算其在随机聚类结果中的排名比原始聚类结果高的比例，高意思就是平均突变率小。排名越高，对应类的平均突变频率越小。
#从而得到对应的p值。如果p值小于设定的显著性水平（通常是0.05），则拒绝原假设，原始聚类结果中该聚类不是随机出现的，否则接受原假设（即该聚类可能是由随机性产生的，没有显著性）

#如果原始数据聚类后的平均突变那一类的p值小于0.05的话，print出该类里的点
# 如果最高平均突变频率的类的p值小于0.05，则显示该类的点
if (p_values[which.max(orig_means)] < 0.05) {
  cat("原始聚类结果中平均突变频率最高的聚类中的点：\n")
  cat(paste("行号\tX\tY\tZ\tvirusPercent\n"))
  selected_rows <- which(orig_clusters$cluster == which.max(orig_means))
  for (i in selected_rows) {
    cat(paste(i, "\t", scale_vector[i, "X"], "\t", scale_vector[i, "Y"], "\t", scale_vector[i, "Z"], "\t", scale_vector[i, "V4"], "\n"))
  }
  
  # 可视化所有点（用蓝色表示）和突出显示的点（用红色表示）
  significant_points <- which(orig_clusters$cluster == which.max(orig_means))  # 提取p值小于0.05的聚类中的所有数据点的索引
  print(significant_clusters)
  print(significant_points)
  # 提取p值小于0.05的聚类中的所有数据点的坐标
  significant_coordinates <- scale_vector[significant_points, c("X", "Y", "Z")]
  
  # 在三维点图中可视化所有数据点和p值小于0.05的聚类中的数据点
  fig <- plot_ly() %>%
    add_trace(data = scale_vector, x = ~X, y = ~Y, z = ~Z, type = "scatter3d", mode = "markers", name = "All Points") %>%
    add_trace(data = significant_coordinates, x = ~X, y = ~Y, z = ~Z, type = "scatter3d", mode = "markers", name = "Significant Clusters", marker = list(color = "red")) %>%
    layout(scene = list(xaxis = list(title = "X"), yaxis = list(title = "Y"), zaxis = list(title = "Z")))
  
  fig
}

#检验点的正确
cluster_data <- scale_vector[orig_clusters$cluster == which.max(orig_means), ]
print(cluster_data)

}

#筛选p值，p值过小的点的k才选中
if (length(k_validated) == 0) {
  temp <- 0
} else {
  # 如果 condition 为 FALSE，则执行这里的代码
  colnames(k_validated) <- c("K")

# 选取 K-num 与 K_validated 相同的行
Final_K <- Best_K[Best_K$`K-num` %in% k_validated$`K`, ]

datatable(Final_K)

}



```



##Determine parameter

For K-means method, we take k = ?


